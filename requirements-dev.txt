# Same as requirements.txt (flash-attn is not in either file; it's installed separately in Dockerfile).
# Dev container doesn't need flash-attn (no nvcc); vLLM will use TORCH_SDPA fallback.

# vLLM and core dependencies (pinned from RunPod working worker 2026-01-26)
vllm==0.12.0
# torch already included in base image
transformers==4.57.3
tokenizers==0.22.1
sentencepiece==0.2.1

# FastAPI and server dependencies
fastapi==0.124.2
uvicorn[standard]==0.38.0
pydantic==2.12.5
aiohttp==3.10.5

# Hugging Face Hub integration
huggingface-hub==0.36.0
datasets==4.4.1

# Additional utilities
numpy==1.26.4
packaging==25.0
psutil==6.0.0
ray==2.52.1

# Optional: For better performance and compatibility
accelerate==1.12.0
safetensors==0.4.5

# flash-attn is NOT here â€” see Dockerfile (production) or skip entirely (dev container).
