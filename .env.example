# Server configuration
PORT=8000
HOST=0.0.0.0
API_KEY=your_api_key_here

# Model configuration
MODEL_ID=meta-llama/Llama-3.2-1B-Instruct

# Hugging Face configuration (for private model access)
HF_TOKEN=your_huggingface_token_here

# Cache and storage
CACHE_DIR=/tmp/.cache/huggingface

# LoRA configuration
MAX_LORAS=10
MAX_LORA_RANK=16
MAX_CPU_LORAS=5

# vLLM configuration
VLLM_ALLOW_RUNTIME_LORA_UPDATING=True