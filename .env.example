# Server configuration
PORT=8000
PORT_HEALTH=8001
HOST=0.0.0.0
API_KEY=your_api_key_here

# Model configuration
MODEL_ID=meta-llama/Llama-3.2-1B-Instruct

# Hugging Face configuration (for private model access)
HF_TOKEN=your_huggingface_token_here

# Build-time model pre-download (optional)
# Set BUILD_MODEL_ID to pre-download a model during Docker image build
# Set BUILD_HF_TOKEN to use HF authentication during build (for private models)
BUILD_MODEL_ID=
BUILD_HF_TOKEN=

# Cache and storage
CACHE_DIR=/workspace/.cache/huggingface

# LoRA configuration
MAX_LORAS=10
MAX_LORA_RANK=16
MAX_CPU_LORAS=25

# vLLM configuration
VLLM_ALLOW_RUNTIME_LORA_UPDATING=True

# Auto tool choice configuration (for function calling)
# Enable automatic tool choice - defaults to true
ENABLE_AUTO_TOOL_CHOICE=true
# Explicitly set tool call parser - if not set, will be inferred from model ID when auto tool choice is enabled
TOOL_CALL_PARSER=

# Chat template configuration
COPY_CHAT_TEMPLATE=true

# Multimodal configuration
# Timeouts for fetching media files via HTTP URLs (in seconds)
IMAGE_FETCH_TIMEOUT=5
VIDEO_FETCH_TIMEOUT=30
AUDIO_FETCH_TIMEOUT=10

# Maximum number of media items allowed per prompt
# Set to 0 to disable that media type
MAX_IMAGES_PER_PROMPT=4
MAX_VIDEOS_PER_PROMPT=1
MAX_AUDIOS_PER_PROMPT=1