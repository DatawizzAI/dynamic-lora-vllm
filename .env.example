# Server configuration
PORT=8000
PORT_HEALTH=8001
HOST=0.0.0.0
API_KEY=your_api_key_here

# Model configuration
MODEL_ID=meta-llama/Llama-3.2-1B-Instruct

# Hugging Face configuration (for private model access)
HF_TOKEN=your_huggingface_token_here

# Build-time model pre-download (optional)
# Set BUILD_MODEL_ID to pre-download a model during Docker image build
# Set BUILD_HF_TOKEN to use HF authentication during build (for private models)
BUILD_MODEL_ID=
BUILD_HF_TOKEN=

# Cache and storage
CACHE_DIR=/workspace/.cache/huggingface

# LoRA configuration
MAX_LORAS=10
MAX_LORA_RANK=16
MAX_CPU_LORAS=25

# vLLM configuration
VLLM_ALLOW_RUNTIME_LORA_UPDATING=True

# Chat template configuration
COPY_CHAT_TEMPLATE=true